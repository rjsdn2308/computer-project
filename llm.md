#  LLM 프로젝트 분석 보고서

팀원 정보 팀원: [20201702] [김건우] [20211704] [이준성] [20231901] [강민수]


(팀원이 더 있으면 이어서 작성)

## 1\. 참고 프로젝트 1

### 1.1.컴퓨터 비전 기반 과일 인식 및 성장 과정 학습 프로그램 (유아 교육용)

### 1.2 프로젝트 목적

- 아이들이 과일 사진을 찍으면 해당 과일을 인식하여, 과일의 이름·원산지·성장 과정 이미지를 보여주고 학습할 수 있도록 돕는 프로그램.  
   → 시각 자료 기반의 맞춤형 교육 제공을 통해 **유아의 흥미와 이해도 향상**을 목표로 함.

### 1.3. LLM 활용 요소 분석

**텍스트 생성 및 요약**

* 컴퓨터비전 모델이 과일을 인식하면, LLM이 과일의 **성장 과정, 주요 영양소, 재배 환경** 등을 설명하는 텍스트를 자동 생성  
* 어린이의 발달 수준에 맞춰, 전문적인 정보를 **짧고 간단한 문장**으로 요약하여 제공  
* 동일한 정보를 아이 연령대별로 다르게 설명 (예: 유치원 아동용 → “사과는 나무에서 자라요 🍎”, 초등학생용 → “사과는 온대 기후에서 잘 자라며, 봄에 꽃이 피고 가을에 열매를 맺습니다”)

**자연어 이해 (질의응답)**

* 아이가 직접 질문을 텍스트/음성으로 입력하면, 해당 질문의 \*\*의도(Intent)\*\*를 파악  
* 예: “사과는 어디서 자라?” → 지역/환경 정보 제공, “바나나는 왜 길어?” → 구조적 특징 설명  
* 질문이 모호할 경우, LLM이 아이가 이해하기 쉽게 **추가 질문을 통해 대화를 이어감**

**다국어 번역 및 언어 적응**

* 한국어뿐 아니라 영어·중국어 등 **다국어로 설명 제공 가능** → 다문화 교육 환경에도 활용  
* 같은 내용을 단순 번역하는 것이 아니라, 문화적 차이를 반영해 **아이에게 친숙한 표현으로 재작성**  
* 예: 영어권 아동에게는 “Apple is a fruit that grows on trees and is often eaten as a snack”처럼 단순 명료하게 전달

**맞춤형 학습 지원**

* 아이의 질문·학습 수준을 기록하고, 그에 따라 **다음 설명이나 추가 학습 자료를 추천**  
* 반복적으로 같은 질문을 하면, LLM이 다른 방식으로 설명하여 **다각적 이해**를 도와줍니다.
  
### 1.4. 역할 분담
- 각자가 프로젝트에 대해서 어떤 모듈 및 프로그램 등 다양한 방법을 사용할 지 알아보고 생각한 후 개인이 생각해서 만들어서 온 프로그램 및 데이테셋을 가져와서 서로 간의 피드백 후 3개 中 하나를 선정해서 프로젝트를 시작할 예정

 ### 1.5. 참고하고 싶은 이유

- 기존의 단순 이미지 분류를 넘어, LLM을 접목하여 **아이 눈높이에 맞는 맞춤형 설명 제공**이 가능

- 컴퓨터 비전 \+ LLM 융합 프로젝트의 대표적인 사례로 참고 가치가 있음


## 2.참고 프로젝트 2

### 2.1. 프로젝트 주제

손모양(제스처) 인식 기반 스마트 제어 시스템 (음악 재생·조명 제어)

### 2.2. 프로젝트 목적

- 카메라를 통해 사용자의 손동작을 인식하여, **제스처에 따라 음악 재생, 조명 켜기/끄기 등의 동작**을 수행하는 스마트 제어 시스템. → 물리적 버튼 조작 없이 직관적인 상호작용 제공.

### 2.3. LLM 활용 요소 분석

* **자연어 이해 (제스처 \+ 명령 결합 해석)**

  * 단순히 손동작만 인식하는 것이 아니라, 사용자가 **음성 명령과 제스처를 동시에 사용했을 때 맥락을 이해**  
  * 예: “이 노래 틀어줘” \+ OK 제스처 → 음악 재생  
  * 예: 손바닥 펼치기 \+ “불 켜줘” → 조명 ON  
  * 즉, LLM이 사용자의 \*\*의도(Intent)\*\*와 \*\*맥락(Context)\*\*을 통합적으로 해석

* **피드백 문장 생성 (상호작용 강화)**  
  * 제스처 인식 후, LLM이 상황에 맞는 자연스러운 응답 문장을 생성  
  * 예: “손가락 브이(V) 제스처 → 음악 일시정지” → LLM이 “네, 음악을 잠시 멈출게요 🎵”라고 안내  
  * 단순한 기계 반응이 아니라 **사람과 대화하는 듯한 피드백** 제공

* **다국어 지원 및 사용자 맞춤화**  
  * 한국어, 영어 등 다국어로 시스템 안내 가능  
  * 예: “Light is turned on 💡” / “조명이 켜졌습니다 💡”  
  * 어린이, 청소년, 성인 등 **사용자 연령·상황에 따라 표현 방식을 조절**  
    * 아이 → “불이 켜졌어\! 방이 밝아졌네 🌟”  
    * 성인 → “조명이 켜졌습니다.”

* **상황별 자동화 스크립트 생성**  
  * 사용자가 원하는 동작을 자연어로 설명하면, LLM이 이를 코드/자동화 규칙으로 변환  
  * 예: “내가 손바닥을 두 번 흔들면 음악이 랜덤으로 바뀌게 해줘” → LLM이 Python/IoT 제어 코드로 자동 작성 및 적용  
  * 새로운 제스처-명령 매핑을 **사용자 맞춤형으로 쉽게 확장** 가능

* **사용자 의도 학습 및 추천 기능**

  * 사용자가 자주 하는 제스처 패턴을 기록하고, LLM이 이를 학습  
  * 예: 매일 밤 손바닥 제스처 → 조명 끄기 → LLM이 “이제부터 밤 10시에 자동으로 불을 꺼드릴까요?” 제안
 
### 2.4.역할 분담

- 각자가 프로젝트에 대해서 어떤 모듈 및 프로그램 등 다양한 방법을 사용할 지 알아보고 생각한 후 개인이 생각해서 만들어서 온 프로그램 및 데이테셋을 가져와서 서로 간의 피드백 후 3개 中 하나를 선정해서 프로젝트를 시작할 예정


### 2.5. 참고할 프로젝트 문제 정의 및 목적

## 1-1. 참고할 데이터셋
- 참고할 데이터셋 : HaGRID - HAnd Gesture Recognition Image Dataset(https://www.kaggle.com/datasets/kapitanov/hagrid/data?select=ann_subsample)

# HaGRID 데이터셋(`ann_subsample`) 정보

## 1. 데이터셋 개요
HaGRID (HAnd Gesture Recognition Image Dataset)은 손 제스처 인식 연구를 위해 제작된 이미지 데이터셋입니다.  
데이터셋은 원본 이미지와 리샘플링된 `ann_subsample` 버전으로 제공됩니다.

## 2. `ann_subsample` 데이터셋

- **파일 크기**: 약 12.5 GB
- **이미지 수**: 507,050장
- **이미지 해상도**: 512p (리샘플링)
- **구성 내용**:
  - 각 이미지에는 손 제스처 바운딩 박스(bounding box) 포함
  - 이미지별 제스처 라벨(annotation) 포함
- **용도**: 원본 HaGRID 데이터셋에서 추출된 대표 샘플로,  
  모델 학습과 실험에 적합하도록 구성됨

## 3. 원본 데이터셋

- **파일 크기**: 약 716 GB
- **이미지 수**: 552,992장
- **이미지 해상도**: Full HD (1920×1080)
- **제스처 클래스**: 18개
- **촬영 환경**:
  - 다양한 조명 조건
  - 다양한 배경
  - 여러 참가자 손 사진 포함

### 2.5. 참고하고 싶은 이유

- 제스처 인식 자체는 컴퓨터비전 기술이지만, **LLM을 활용해 사용자 의도 해석 및 직관적인 피드백**을 제공할 수 있음

- IoT와 결합된 **스마트 홈/스마트 클래스룸 시스템**으로 확장 가능성이 큼




